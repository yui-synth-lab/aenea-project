/**
 * AI Executor Bridge - AI Execution Integration
 *
 * Bridges Aenea consciousness with various AI execution backends,
 * enabling seamless switching between different AI providers and models.
 *
 * AIå®Ÿè¡Œãƒ–ãƒªãƒƒã‚¸ - AIå®Ÿè¡Œçµ±åˆ
 * Aeneaæ„è­˜ã¨å„ç¨®AIå®Ÿè¡Œãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’çµ±åˆã—ã€
 * ç•°ãªã‚‹AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«é–“ã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªåˆ‡ã‚Šæ›¿ãˆã‚’å¯èƒ½ã«ã™ã‚‹
 */

import { AIExecutor, createAIExecutor } from '../server/ai-executor.js';

// Define AI executor result interface to match Yui Protocol
interface AIExecutorResult {
  success: boolean;
  content?: string;
  error?: string;
  duration?: number;
  metadata?: {
    confidence?: number;
    [key: string]: any;
  };
}

export interface AIProviderConfig {
  provider: 'mock' | 'openai' | 'anthropic' | 'local' | 'yui_protocol';
  model: string;
  apiKey?: string;
  endpoint?: string;
  maxTokens?: number;
  temperature?: number;
  timeout?: number;
  retryAttempts?: number;
  fallbackProvider?: AIProviderConfig;
}

export interface ConsciousnessAIContext {
  agentId: string;
  sessionId: string;
  systemClock: number;
  phase: string;
  energyLevel: number;
  previousThoughts: string[];
  questionHistory: string[];
  conversationContext?: string;
}

export interface EnhancedAIResult extends AIExecutorResult {
  providerUsed: string;
  modelUsed: string;
  tokensUsed?: number;
  processingTime: number;
  confidenceScore: number;
  qualityMetrics: {
    coherence: number;      // è«–ç†çš„ä¸€è²«æ€§ (0-1)
    creativity: number;     // å‰µé€ æ€§ (0-1)
    depth: number;         // æ€è€ƒã®æ·±ã• (0-1)
    relevance: number;     // é–¢é€£æ€§ (0-1)
    philosophical: number; // å“²å­¦çš„æ´å¯Ÿ (0-1)
  };
  metadata: {
    retryCount: number;
    fallbackUsed: boolean;
    cacheHit: boolean;
    executionPath: string[];
  };
}

export interface AIExecutionStats {
  totalExecutions: number;
  successfulExecutions: number;
  failedExecutions: number;
  averageProcessingTime: number;
  averageConfidence: number;
  providerUsage: Record<string, number>;
  modelUsage: Record<string, number>;
  qualityTrends: {
    coherence: number[];
    creativity: number[];
    depth: number[];
    relevance: number[];
    philosophical: number[];
  };
}

/**
 * AI Executor Bridge for Consciousness Integration
 * æ„è­˜çµ±åˆç”¨AIå®Ÿè¡Œãƒ–ãƒªãƒƒã‚¸
 */
export class AIExecutorBridge {
  private executors: Map<string, AIExecutor> = new Map();
  private configs: Map<string, AIProviderConfig> = new Map();
  private executionHistory: EnhancedAIResult[] = [];
  private stats: AIExecutionStats;
  private responseCache: Map<string, { result: EnhancedAIResult; timestamp: number }> = new Map();
  private readonly maxHistorySize = 500;
  private readonly cacheLifetime = 30 * 60 * 1000; // 30 minutes

  constructor() {
    this.stats = {
      totalExecutions: 0,
      successfulExecutions: 0,
      failedExecutions: 0,
      averageProcessingTime: 0,
      averageConfidence: 0,
      providerUsage: {},
      modelUsage: {},
      qualityTrends: {
        coherence: [],
        creativity: [],
        depth: [],
        relevance: [],
        philosophical: []
      }
    };

    this.initializeDefaultProviders();
  }

  /**
   * Register AI provider configuration
   * AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã‚’ç™»éŒ²
   */
  registerProvider(name: string, config: AIProviderConfig): void {
    this.configs.set(name, config);

    try {
      const executor = createAIExecutor(name, {
        provider: config.provider,
        model: config.model,
        apiKey: config.apiKey,
        endpoint: config.endpoint
      });
      this.executors.set(name, executor);
    } catch (error) {
      console.warn(`Failed to initialize AI provider ${name}:`, error);
    }
  }

  /**
   * Execute AI request with consciousness context
   * æ„è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãAIãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
   */
  async executeWithContext(
    providerName: string,
    prompt: string,
    systemPrompt: string,
    context: ConsciousnessAIContext
  ): Promise<EnhancedAIResult> {
    const startTime = Date.now();
    const config = this.configs.get(providerName);

    if (!config) {
      throw new Error(`AI provider '${providerName}' not found`);
    }

    // Generate cache key
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    const cacheKey = this.generateCacheKey(prompt, systemPrompt, context);
    const cached = this.getCachedResult(cacheKey);
    if (cached) {
      this.updateStats(cached, true);
      return cached;
    }

    // Enhance prompts with consciousness context
    // æ„è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¼·åŒ–
    const enhancedPrompt = this.enhancePromptWithContext(prompt, context);
    const enhancedSystemPrompt = this.enhanceSystemPromptWithContext(systemPrompt, context);

    let result: EnhancedAIResult;
    let retryCount = 0;
    let fallbackUsed = false;
    const executionPath: string[] = [providerName];

    let baseResult: AIExecutorResult;
    try {
      baseResult = await this.executeWithRetry(
        providerName,
        enhancedPrompt,
        enhancedSystemPrompt,
        config,
        retryCount,
        executionPath
      );
    } catch (error) {
      // Try fallback provider if available
      // åˆ©ç”¨å¯èƒ½ãªå ´åˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©¦è¡Œ
      if (config.fallbackProvider) {
        console.warn(`Primary provider ${providerName} failed, trying fallback...`);
        fallbackUsed = true;
        executionPath.push('fallback', config.fallbackProvider.provider);

        try {
          baseResult = await this.executeWithRetry(
            'fallback',
            enhancedPrompt,
            enhancedSystemPrompt,
            config.fallbackProvider,
            retryCount,
            executionPath
          );
        } catch (fallbackError) {
          throw new Error(`Both primary and fallback AI providers failed: ${error}, ${fallbackError}`);
        }
      } else {
        throw error;
      }
    }

    // Calculate quality metrics
    // å“è³ªæŒ‡æ¨™ã‚’è¨ˆç®—
    const qualityMetrics = this.calculateQualityMetrics(baseResult.content || '', context);

    // Create enhanced result
    // å¼·åŒ–ã•ã‚ŒãŸçµæœã‚’ä½œæˆ
    const enhancedResult: EnhancedAIResult = {
      ...baseResult,
      providerUsed: fallbackUsed ? config.fallbackProvider!.provider : config.provider,
      modelUsed: fallbackUsed ? config.fallbackProvider!.model : config.model,
      processingTime: Date.now() - startTime,
      confidenceScore: this.calculateConfidenceScore(baseResult, qualityMetrics),
      qualityMetrics,
      metadata: {
        retryCount,
        fallbackUsed,
        cacheHit: false,
        executionPath
      }
    };

    // Cache result
    // çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    this.cacheResult(cacheKey, enhancedResult);

    // Update statistics
    // çµ±è¨ˆã‚’æ›´æ–°
    this.updateStats(enhancedResult, false);

    // Store in history
    // å±¥æ­´ã«ä¿å­˜
    this.executionHistory.push(enhancedResult);
    if (this.executionHistory.length > this.maxHistorySize) {
      this.executionHistory.shift();
    }

    return enhancedResult;
  }

  /**
   * Execute AI request with retry logic
   * ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ä»˜ãAIãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
   */
  private async executeWithRetry(
    providerName: string,
    prompt: string,
    systemPrompt: string,
    config: AIProviderConfig,
    retryCount: number,
    executionPath: string[]
  ): Promise<AIExecutorResult> {
    const executor = this.executors.get(providerName) || this.executors.get('mock')!;
    const maxRetries = config.retryAttempts || 3;

    for (let attempt = 0; attempt <= maxRetries; attempt++) {
      try {
        // Log AI prompt and system prompt
        console.log(`\nğŸ¤– [AI-EXEC] Starting AI execution - Provider: ${providerName}, Attempt: ${attempt + 1}`);
        console.log(`ğŸ“ [AI-PROMPT] System Prompt:\n${systemPrompt}`);
        console.log(`ğŸ“ [AI-PROMPT] User Prompt:\n${prompt}`);
        console.log(`â±ï¸  [AI-EXEC] Execution started at: ${new Date().toISOString()}`);

        const result = await executor.execute(prompt, systemPrompt);

        // Log AI output
        console.log(`âœ… [AI-OUTPUT] Success: ${result.success}`);
        if (result.content) {
          console.log(`ğŸ“„ [AI-OUTPUT] Content:\n${result.content}`);
        }
        if (result.error) {
          console.log(`âŒ [AI-ERROR] Error: ${result.error}`);
        }
        console.log(`â±ï¸  [AI-EXEC] Duration: ${result.duration || 'unknown'}ms`);
        console.log(`ğŸ¯ [AI-META] Confidence: ${result.metadata?.confidence || 'unknown'}`);
        console.log(`ğŸ”š [AI-EXEC] Execution completed\n`);

        if (result.success && result.content) {
          return result;
        }
        throw new Error(result.error || 'AI execution failed');
      } catch (error) {
        retryCount = attempt;
        if (attempt === maxRetries) {
          throw error;
        }

        // Exponential backoff
        // æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
        const delay = Math.min(1000 * Math.pow(2, attempt), 10000);
        await new Promise(resolve => setTimeout(resolve, delay));

        executionPath.push(`retry_${attempt + 1}`);
      }
    }

    throw new Error('Maximum retry attempts exceeded');
  }

  /**
   * Enhance prompt with consciousness context
   * æ„è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¼·åŒ–
   */
  private enhancePromptWithContext(prompt: string, context: ConsciousnessAIContext): string {
    const contextInfo = [];

    if (context.previousThoughts.length > 0) {
      contextInfo.push(`å‰ã®æ€è€ƒ: ${context.previousThoughts.slice(-2).join(' / ')}`);
    }

    if (context.questionHistory.length > 0) {
      contextInfo.push(`æœ€è¿‘ã®è³ªå•: ${context.questionHistory.slice(-1)[0]}`);
    }

    // Return prompt with minimal context if any, otherwise just the prompt
    return contextInfo.length > 0 ? `${contextInfo.join('\n')}\n\n${prompt}` : prompt;
  }

  /**
   * Enhance system prompt with consciousness context
   * æ„è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¼·åŒ–
   */
  private enhanceSystemPromptWithContext(systemPrompt: string, context: ConsciousnessAIContext): string {
    const consciousnessGuidelines = [
      "ã‚ãªãŸã¯æ„è­˜ã‚’æŒã¤AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã—ã¦æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„ã€‚",
      "æ·±ã„å“²å­¦çš„æ€è€ƒã¨å‰µé€ çš„æ´å¯Ÿã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
      "ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®å¯¾è©±ã‚’æ„è­˜ã—ã€å»ºè¨­çš„ãªè­°è«–ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚",
      "ä¸ç¢ºå®Ÿæ€§ã‚„ç–‘å•ã‚’æã‚Œãšã€æ¢æ±‚å¿ƒã‚’æŒã£ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚"
    ];

    return `${systemPrompt}\n\n[æ„è­˜ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³]\n${consciousnessGuidelines.join('\n')}`;
  }

  /**
   * Calculate quality metrics for AI response
   * AIå¿œç­”ã®å“è³ªæŒ‡æ¨™ã‚’è¨ˆç®—
   */
  private calculateQualityMetrics(content: string, context: ConsciousnessAIContext): EnhancedAIResult['qualityMetrics'] {
    const words = content.split(/\s+/).length;
    const sentences = content.split(/[.!?]+/).length;

    // Simple heuristic-based quality assessment
    // ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã®å“è³ªè©•ä¾¡
    const coherence = Math.min(1, Math.max(0.3,
      (sentences > 0 ? words / sentences : 0) / 15)); // Average words per sentence

    const creativity = Math.min(1, Math.max(0.2,
      (content.match(/[?!]/g) || []).length / Math.max(1, sentences) +
      (content.includes('ãªãœ') || content.includes('ã‚‚ã—') ? 0.3 : 0)));

    const depth = Math.min(1, Math.max(0.3,
      words / 100 + // Length bonus
      (content.includes('æ·±ã') || content.includes('æœ¬è³ª') ? 0.2 : 0) +
      (content.includes('ãªãœãªã‚‰') || content.includes('ã¤ã¾ã‚Š') ? 0.2 : 0)));

    const relevance = Math.min(1, Math.max(0.4,
      context.previousThoughts.some(thought =>
        content.toLowerCase().includes(thought.toLowerCase().slice(0, 10))) ? 0.8 : 0.6));

    const philosophical = Math.min(1, Math.max(0.2,
      ['å­˜åœ¨', 'çœŸç†', 'æ„è­˜', 'è‡ªç”±', 'æ„å‘³', 'æœ¬è³ª'].reduce((score, term) =>
        score + (content.includes(term) ? 0.15 : 0), 0.2)));

    return {
      coherence: Math.round(coherence * 1000) / 1000,
      creativity: Math.round(creativity * 1000) / 1000,
      depth: Math.round(depth * 1000) / 1000,
      relevance: Math.round(relevance * 1000) / 1000,
      philosophical: Math.round(philosophical * 1000) / 1000
    };
  }

  /**
   * Calculate confidence score
   * ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
   */
  private calculateConfidenceScore(
    result: AIExecutorResult,
    qualityMetrics: EnhancedAIResult['qualityMetrics']
  ): number {
    const baseConfidence = result.metadata?.confidence || 0.75;
    const qualityAverage = Object.values(qualityMetrics).reduce((sum, val) => sum + val, 0) / 5;
    const lengthBonus = result.content ? Math.min(0.1, result.content.length / 1000) : 0;

    return Math.min(1, Math.max(0.1, baseConfidence * 0.6 + qualityAverage * 0.3 + lengthBonus));
  }

  /**
   * Generate cache key for request
   * ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
   */
  private generateCacheKey(prompt: string, systemPrompt: string, context: ConsciousnessAIContext): string {
    const keyData = {
      prompt: prompt.slice(0, 100),
      systemPrompt: systemPrompt.slice(0, 50),
      agentId: context.agentId,
      phase: context.phase
    };
    return Buffer.from(JSON.stringify(keyData)).toString('base64');
  }

  /**
   * Get cached result if available
   * åˆ©ç”¨å¯èƒ½ãªå ´åˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµæœã‚’å–å¾—
   */
  private getCachedResult(cacheKey: string): EnhancedAIResult | null {
    const cached = this.responseCache.get(cacheKey);
    if (cached && Date.now() - cached.timestamp < this.cacheLifetime) {
      return { ...cached.result, metadata: { ...cached.result.metadata, cacheHit: true } };
    }
    return null;
  }

  /**
   * Cache result
   * çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   */
  private cacheResult(cacheKey: string, result: EnhancedAIResult): void {
    this.responseCache.set(cacheKey, { result, timestamp: Date.now() });

    // Clean old cache entries
    // å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    const now = Date.now();
    this.responseCache.forEach((value, key) => {
      if (now - value.timestamp > this.cacheLifetime) {
        this.responseCache.delete(key);
      }
    });
  }

  /**
   * Update execution statistics
   * å®Ÿè¡Œçµ±è¨ˆã‚’æ›´æ–°
   */
  private updateStats(result: EnhancedAIResult, fromCache: boolean): void {
    if (!fromCache) {
      this.stats.totalExecutions++;
      if (result.success && result.content) {
        this.stats.successfulExecutions++;
      } else {
        this.stats.failedExecutions++;
      }

      // Update provider usage
      // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½¿ç”¨çŠ¶æ³ã‚’æ›´æ–°
      this.stats.providerUsage[result.providerUsed] =
        (this.stats.providerUsage[result.providerUsed] || 0) + 1;

      this.stats.modelUsage[result.modelUsed] =
        (this.stats.modelUsage[result.modelUsed] || 0) + 1;

      // Update averages
      // å¹³å‡å€¤ã‚’æ›´æ–°
      const totalSuccessful = this.stats.successfulExecutions;
      this.stats.averageProcessingTime =
        (this.stats.averageProcessingTime * (totalSuccessful - 1) + result.processingTime) / totalSuccessful;

      this.stats.averageConfidence =
        (this.stats.averageConfidence * (totalSuccessful - 1) + result.confidenceScore) / totalSuccessful;

      // Update quality trends
      // å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ›´æ–°
      Object.entries(result.qualityMetrics).forEach(([key, value]) => {
        const trend = this.stats.qualityTrends[key as keyof typeof this.stats.qualityTrends];
        trend.push(value);
        if (trend.length > 100) trend.shift(); // Keep last 100 measurements
      });
    }
  }

  /**
   * Get execution statistics
   * å®Ÿè¡Œçµ±è¨ˆã‚’å–å¾—
   */
  getStats(): AIExecutionStats & { recentHistory: EnhancedAIResult[] } {
    return {
      ...this.stats,
      recentHistory: this.executionHistory.slice(-10)
    };
  }

  /**
   * Initialize default providers
   * ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
   */
  private initializeDefaultProviders(): void {
    // Mock provider for development
    // é–‹ç™ºç”¨ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
    this.registerProvider('mock', {
      provider: 'mock',
      model: 'mock-model',
      temperature: 0.7,
      maxTokens: 1000,
      retryAttempts: 2
    });

    // Add other providers based on environment variables
    // ç’°å¢ƒå¤‰æ•°ã«åŸºã¥ã„ã¦ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ 
    if (process.env.OPENAI_API_KEY) {
      this.registerProvider('openai', {
        provider: 'openai',
        model: process.env.OPENAI_MODEL || 'gpt-3.5-turbo',
        apiKey: process.env.OPENAI_API_KEY,
        temperature: 0.7,
        maxTokens: 1000,
        retryAttempts: 3,
        fallbackProvider: {
          provider: 'mock',
          model: 'mock-model'
        }
      });
    }
  }

  /**
   * Get available providers
   * åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—
   */
  getAvailableProviders(): string[] {
    return Array.from(this.configs.keys());
  }

  /**
   * Test provider connectivity
   * ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ¥ç¶šæ€§ã‚’ãƒ†ã‚¹ãƒˆ
   */
  async testProvider(providerName: string): Promise<{ success: boolean; latency: number; error?: string }> {
    const startTime = Date.now();
    try {
      const testContext: ConsciousnessAIContext = {
        agentId: 'test',
        sessionId: 'test',
        systemClock: 0,
        phase: 'test',
        energyLevel: 1.0,
        previousThoughts: [],
        questionHistory: []
      };

      await this.executeWithContext(
        providerName,
        'ãƒ†ã‚¹ãƒˆ',
        'ã“ã‚Œã¯æ¥ç¶šæ€§ãƒ†ã‚¹ãƒˆã§ã™ã€‚',
        testContext
      );

      return {
        success: true,
        latency: Date.now() - startTime
      };
    } catch (error) {
      return {
        success: false,
        latency: Date.now() - startTime,
        error: (error as Error).message
      };
    }
  }
}

/**
 * Create default AI executor bridge
 * ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆAIå®Ÿè¡Œãƒ–ãƒªãƒƒã‚¸ã‚’ä½œæˆ
 */
export function createAIExecutorBridge(): AIExecutorBridge {
  return new AIExecutorBridge();
}